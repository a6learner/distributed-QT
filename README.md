# ğŸ§  Distributed Q-Transformer (DQT)
**Implementation of â€œScalability and Noise Resilience in Q-Transformerâ€ (Wang, 2024)**
**åˆ†å¸ƒå¼ Q-Transformerï¼šå¯æ‰©å±•æ€§ä¸é²æ£’æ€§ç ”ç©¶å®ç°**
ğŸ“„ [è®ºæ–‡é“¾æ¥](https://openreview.net/pdf?id=WQupWGepAO)
---

## ğŸ“˜ Overview | é¡¹ç›®ç®€ä»‹

This project is built upon [lucidrains/q-transformer](https://github.com/lucidrains/q-transformer),

extending its core architecture to a distributed multi-agent setting with enhanced noise robustness and scalability analysis.

---

## ğŸš€ Quick Start | å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ Setup Environment / ç¯å¢ƒé…ç½®
```bash
git clone https://github.com/<username>/Distributed-QT.git
cd Distributed-QT
conda env create -f environment.yml
conda activate dqt
```

### 2ï¸âƒ£ Run Distributed Training / è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
```bash
python src/main.py --num_agents 4 --train_steps 300000 --task mw-door-unlock
```

### 3ï¸âƒ£ Run Noise Robustness Test / å™ªå£°é²æ£’æ€§æµ‹è¯•
```bash
python scripts/run_noise_experiments.py --noise_mean 0 --noise_variance 1.0
```

All logs and visual outputs will be generated under `experiment_logs/` and `outputs/`.

---

## ğŸ“‚ Directory Structure | é¡¹ç›®ç»“æ„

```
DISTRIBUTED-QT/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ distributed/     # åˆ†å¸ƒå¼æ ¸å¿ƒæ¨¡å—ï¼ˆagent / server / trainerï¼‰
â”‚   â”œâ”€â”€ QTransformer.py  # ä¸»æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ main.py          # è®­ç»ƒå…¥å£
â”‚   â””â”€â”€ config.yaml      # é…ç½®æ–‡ä»¶
â”œâ”€â”€ scripts/             # å®éªŒè„šæœ¬ä¸å¯è§†åŒ–
â”œâ”€â”€ conclusion/          # å®éªŒç»“è®ºä¸å›¾è¡¨
â”œâ”€â”€ experiment_logs/     # æ—¥å¿—ä¸æŒ‡æ ‡
â”œâ”€â”€ outputs/             # è¾“å‡ºç»“æœ
â””â”€â”€ environment.yml
```

---

## ğŸ§ª Experiments | å®éªŒç®€è¿°

We conducted controlled experiments on **MetaWorld Door-Unlock**, testing scalability across multiple agents and evaluating robustness under noisy rewards.
æœ¬ç ”ç©¶åœ¨ **MetaWorld Door-Unlock ä»»åŠ¡** ä¸Šè¿›è¡Œç³»ç»Ÿæµ‹è¯•ï¼Œæ¶‰åŠä¸åŒæ™ºèƒ½ä½“æ•°é‡å’Œå™ªå£°ç¯å¢ƒè®¾ç½®ã€‚

- **Distributed Training:**
  Compared configurations with 1, 4, 7, and 11 agents under identical settings.
  Multi-agent setups (esp. 7 agents) showed **faster convergence** and **more stable reward curves** than single-agent baselines.

- **Noise Robustness:**
  Added Gaussian noise (mean 0â€“10, variance 0.1â€“100) to rewards.
  Median-based filtering effectively reduced performance degradation under moderate noise (variance â‰¤ 1.0).

- **Learning Rate Schedules:**
  Evaluated linear, log curve, and adaptive decay strategies.
  Log-based decay yielded **smooth convergence** across varying agent counts.

Overall, DQT achieved **higher stability and sample efficiency**, confirming the effectiveness of distributed and robust training strategies.
æ€»ä½“ç»“æœè¡¨æ˜ï¼Œåˆ†å¸ƒå¼ Q-Transformer åœ¨**æ”¶æ•›é€Ÿåº¦**ã€**ç¨³å®šæ€§**åŠ**é²æ£’æ€§**æ–¹é¢å‡ä¼˜äºå•æ™ºèƒ½ä½“ã€‚

---


âœ… **Summary / æ€»ç»“ï¼š**
Distributed Q-Transformer demonstrates **efficient scalable training** and **robust performance under noise**, offering a practical framework for real-world reinforcement learning.
åˆ†å¸ƒå¼ Q-Transformer åœ¨**å¯æ‰©å±•è®­ç»ƒä¸å™ªå£°é²æ£’æ€§**æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä¸ºå¼ºåŒ–å­¦ä¹ çš„å·¥ç¨‹åŒ–åº”ç”¨æä¾›äº†å¯è¡ŒèŒƒå¼ã€‚
