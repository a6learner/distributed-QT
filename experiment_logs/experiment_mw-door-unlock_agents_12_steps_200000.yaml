duration_seconds: 89.5669903755188
end_time: '2025-01-25 13:40:38'
parameters:
  num_agents: 12
  task: mw-door-unlock
  train_steps: 200000
start_time: '2025-01-25 13:39:09'
status: failure
stderr: "Error executing job with overrides: ['distributed.num_agents=12', 'env.train_steps=200000',\
  \ 'env.task=mw-door-unlock']\nTraceback (most recent call last):\n  File \"/home/wang/project/Distributed_Q-Transformer/src/main.py\"\
  , line 23, in main\n    trainer.run()\n  File \"/home/wang/project/Distributed_Q-Transformer/src/distributed/trainer.py\"\
  , line 118, in run\n    update = self.server.update_networks(batch_experiences)\n\
  \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wang/project/Distributed_Q-Transformer/src/distributed/server.py\"\
  , line 56, in update_networks\n    loss.backward()\n  File \"/home/wang/anaconda3/envs/qt/lib/python3.11/site-packages/torch/_tensor.py\"\
  , line 525, in backward\n    torch.autograd.backward(\n  File \"/home/wang/anaconda3/envs/qt/lib/python3.11/site-packages/torch/autograd/__init__.py\"\
  , line 267, in backward\n    _engine_run_backward(\n  File \"/home/wang/anaconda3/envs/qt/lib/python3.11/site-packages/torch/autograd/graph.py\"\
  , line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(\
  \  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB. GPU\
  \ \n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
